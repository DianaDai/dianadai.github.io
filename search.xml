<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F15%2F%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[1、2PC（两阶段提交），又叫做 XA Transactions第一阶段：事务协调器要求每个涉及到事务的数据库预提交(precommit)此操作，并反映是否可以提交.第二阶段：事务协调器要求每个数据库提交数据。 两阶段提交这种解决方案属于牺牲了一部分可用性来换取的一致性优点： 尽量保证了数据的强一致，适合对数据强一致要求很高的关键领域。（其实也不能100%保证强一致）缺点： 实现复杂，牺牲了可用性，对性能影响较大，不适合高并发高性能场景，如果分布式系统跨接口调用，目前 .NET 界还没有实现方案。2、补偿事务（TCC）TCC 其实就是采用的补偿机制，其核心思想是：针对每个操作，都要注册一个与其对应的确认和补偿（撤销）操作。它分为三个阶段：Try 阶段主要是对业务系统做检测及资源预留Confirm 阶段主要是对业务系统做确认提交，Try阶段执行成功并开始执行 Confirm阶段时，默认 Confirm阶段是不会出错的。即：只要Try成功，Confirm一定成功。Cancel 阶段主要是在业务执行错误，需要回滚的状态下执行的业务取消，预留资源释放。优点： 跟2PC比起来，实现以及流程相对简单了一些，但数据的一致性比2PC也要差一些缺点： 缺点还是比较明显的，在2,3步中都有可能失败。TCC属于应用层的一种补偿方式，所以需要程序员在实现的时候多写很多补偿的代码，在一些场景中，一些业务流程可能用TCC不太好定义及处理。3、本地消息表（异步确保）本地消息表这种实现方式应该是业界使用最多的，其核心思想是将分布式事务拆分成本地事务进行处理基本思路就是：消息生产方，需要额外建一个消息表，并记录消息发送状态。消息表和业务数据要在一个事务里提交，也就是说他们要在一个数据库里面。然后消息会经过MQ发送到消息的消费方。如果消息发送失败，会进行重试发送。 消息消费方，需要处理这个消息，并完成自己的业务逻辑。此时如果本地事务处理成功，表明已经处理成功了，如果处理失败，那么就会重试执行。如果是业务上面的失败，可以给生产方发送一个业务补偿消息，通知生产方进行回滚等操作。 生产方和消费方定时扫描本地消息表，把还没处理完成的消息或者失败的消息再发送一遍。如果有靠谱的自动对账补账逻辑，这种方案还是非常实用的。优点： 一种非常经典的实现，避免了分布式事务，实现了最终一致性。 缺点： 消息表会耦合到业务系统中，如果没有封装好的解决方案，会有很多杂活需要处理。4、基于消息队列的解决方案 https://www.cnblogs.com/bigben0123/p/9453830.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[how to configuration shadowsocks]]></title>
    <url>%2F2019%2F03%2F07%2F1%2F</url>
    <content type="text"><![CDATA[1、基于centos 进行配置执行以下三个命令：1234wget --no-check-certificate -O shadowsocks-libev.sh https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks-libev.shchmod +x shadowsocks-libev.sh./shadowsocks-libev.sh 2&gt;&amp;1 | tee shadowsocks-libev.log 2、启动停止命令123456./shadowsocks-libev.sh uninstall/etc/init.d/shadowsocks status启动：/etc/init.d/shadowsocks start停止：/etc/init.d/shadowsocks stop重启：/etc/init.d/shadowsocks restart状态：/etc/init.d/shadowsocks status 3、Shadowsocks-R以及其他版本https://pdf-lib.org/Home/Details/1182 http://madaimeng.com/article/ShadowSocks/ Shadowsocks-R 是项目 shadowsocks 的增强版，用于方便地产生各种协议接口。实现为在原来的协议外套一层编码和解码接口，不但可以伪装成其它协议流量，还可以把原协议转换为其它协议进行兼容或完善，需要服务端与客户端配置相同的协议插件。 Shadowsocks-Python 版：/etc/init.d/shadowsocks-python start | stop | restart | status ShadowsocksR 版：/etc/init.d/shadowsocks-r start | stop | restart | status Shadowsocks-Go 版：/etc/init.d/shadowsocks-go start | stop | restart | status Shadowsocks-libev 版：/etc/init.d/shadowsocks-libev start | stop | restart | status]]></content>
      <categories>
        <category>shadowsocks</category>
      </categories>
      <tags>
        <tag>shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive 命令行]]></title>
    <url>%2F2019%2F02%2F22%2F1%2F</url>
    <content type="text"><![CDATA[hive shell 介绍https://www.cnblogs.com/qingyunzong/p/8847532.html hive service cli：是Command Line Interface 的缩写，是Hive的命令行界面，用的比较多，是默认服务，直接可以在命令行里使用 hiveserver：这个可以让Hive以提供Thrift服务的服务器形式来运行，可以允许许多个不同语言编写的客户端进行通信，使用需要启动HiveServer服务以和客户端联系，我们可以通过设置HIVE_PORT环境变量来设置服务器所监听的端口，在默认情况下，端口号为10000，这个可以通过以下方式来启动Hiverserver： 1bin/hive --service hiveserver -p 10002 其中-p参数也是用来指定监听端口的 hwi：其实就是hive web interface的缩写它是hive的web借口，是hive cli的一个web替代方案 jar：与hadoop jar等价的Hive接口，这是运行类路径中同时包含Hadoop 和Hive类的Java应用程序的简便方式 metastore：在默认的情况下，metastore和hive服务运行在同一个进程中，使用这个服务，可以让metastore作为一个单独的进程运行，我们可以通过METASTOE——PORT来指定监听的端口号Hive的三种启动方式 hive命令行模式进入hive安装目录，输入bin/hive的执行程序，或者输入 hive –service cli用于linux平台命令行查询，查询语句基本跟mysql查询语句类似 hive web界面的启动方式 1bin/hive –service hwi （&amp; 表示后台运行） 用于通过浏览器来访问hive，感觉没多大用途，浏览器访问地址是：127.0.0.1:9999/hwi hive 远程服务 (端口号10000) 启动方式1bin/hive –service hiveserver2 &amp;（&amp;表示后台运行） 用java，python等程序实现通过jdbc等驱动的访问hive就用这种起动方式了，这个是程序员最需要的方式了 HiveServer/HiveServer2两者都允许远程客户端使用多种编程语言，通过HiveServer或者HiveServer2，客户端可以在不启动CLI的情况下对Hive中的数据进行操作，连这个和都允许远程客户端使用多种编程语言如java，python等向hive提交请求，取回结果（从hive0.15起就不再支持hiveserver了），但是在这里我们还是要说一下hiveserver HiveServer或者HiveServer2都是基于Thrift的，但HiveSever有时被称为Thrift server，而HiveServer2却不会。既然已经存在HiveServer，为什么还需要HiveServer2呢？这是因为HiveServer不能处理多于一个客户端的并发请求，这是由于HiveServer使用的Thrift接口所导致的限制，不能通过修改HiveServer的代码修正。因此在Hive-0.11.0版本中重写了HiveServer代码得到了HiveServer2，进而解决了该问题。HiveServer2支持多客户端的并发和认证，为开放API客户端如JDBC、ODBC提供更好的支持。 Hiveserver1 和hiveserver2的JDBC区别： version Connection Driver HiveServer2 jdbc:hive2://: org.apache.hive.jdbc.HiveDriver HiveServer1 jdbc:hive://: org.apache.hadoop.hive.jdbc.HiveDriver http://www.voidcn.com/article/p-ptluiidz-ev.html]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive架构]]></title>
    <url>%2F2019%2F02%2F18%2F1%2F</url>
    <content type="text"><![CDATA[为什么使用 Hive直接使用 MapReduce 所面临的问题： 人员学习成本太高 项目周期要求太短 MapReduce实现复杂查询逻辑开发难度太大 为什么要使用 Hive： 更友好的接口：操作接口采用类 SQL 的语法，提供快速开发的能力 更低的学习成本：避免了写 MapReduce，减少开发人员的学习成本 更好的扩展性：可自由扩展集群规模而无需重启服务，还支持用户自定义函数Hive 特点优点： 可扩展性,横向扩展，Hive 可以自由的扩展集群的规模，一般情况下不需要重启服务 横向扩展：通过分担压力的方式扩展集群的规模 纵向扩展：一台服务器cpu i7-6700k 4核心8线程，8核心16线程，内存64G =&gt; 128G 延展性，Hive 支持自定义函数，用户可以根据自己的需求来实现自己的函数 良好的容错性，可以保障即使有节点出现问题，SQL 语句仍可完成执行 缺点： Hive 不支持记录级别的增删改操作，但是用户可以通过查询生成新表或者将查询结 果导入到文件中（当前选择的 hive-2.3.2 的版本支持记录级别的插入操作） Hive 的查询延时很严重，因为 MapReduce Job 的启动过程消耗很长时间，所以不能 用在交互查询系统中。 Hive 不支持事务（因为不没有增删改，所以主要用来做 OLAP（联机分析处理），而 不是 OLTP（联机事务处理），这就是数据处理的两大级别）。 Hive的架构 hive的架构中有四个部分组成： 用户接口： - CLI（command line interface），shell终端命令行，采用交互式使用hive命令行与hive进行交互，最常用（学习、生成、调试） - Jdbc/odbc：是hive的基于jdbc操作提供的客户端，用户（开发、运维）通过这个链接hive server服务 - Web UI：通过浏览器访问hive（基本不用） Thrift Server：Thrift是facebook开发的一个软件框架，可以用来进行可扩展且跨语言的服务的开发，hive集成了该服务，能让不同的编程语言调用hive的接口。 底层四大组件：底层的四大组件完成hql查询语句从词法分析，语法分析，编译，优化，以及生成逻辑执行计划的生成。生成的逻辑执行计划存储在hdfs中，并随后由MapReduce调用执行。 - 解释器：解释器的作用是将hiveSQL语句转换成抽象语法数 - 编译器：编译器是将语法树编译成为逻辑执行计划 - 优化器：优化器是对逻辑执行计划进行优化 - 执行器：执行时调用底层的运行框架执行逻辑执行计划执行流程就是：hiveQL，通过命令或者客户端提交，经过compiler编译器，运用metastore中的元数据进行类型检测和语法分析，生成一个逻辑方案，然后通过的优化处理，产生一个maptask程序。 元数据库 ：就是存储在hive中的数据的描述信息，通常包括：表的名字、表的列和分区以及其属性、表的属性（内部表和外部表），表的数据所在目录。而hive有两种元数据的存储方案： - Metastore默认存储在自带的derby数据库中。缺点是：不适合多用户操作，并且数据存储目录不固定。数据库跟着hive的进入目录走，极度不方便管理。 - Hive和mysql之间通过Metastore服务交互（本地或者远程）]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive安装]]></title>
    <url>%2F2019%2F02%2F15%2Fhive%20%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[前言Hive是一种建立在Hadoop文件系统上的数据仓库架构，并对存储在HDFS中的数据进行分析和管理。 Hive 是一个基于 hadoop 的开源数据仓库工具，用于存储和处理海量结构化数据。它把海量数据存储于 hadoop 文件系统，而不是数据库，但提供了一套类数据库的数据存储和处理机制，并采用 HQL （类 SQL ）语言对这些数据进行自动化管理和处理。我们可以把 Hive 中海量结构化数据看成一个个的表，而实际上这些数据是分布式存储在 HDFS 中的。 Hive 经过对语句进行解析和转换，最终生成一系列基于 hadoop 的 map/reduce 任务，通过执行这些任务完成数据处理。 简单来说，Hive就是在Hadoop上架了一层SQL接口，可以将SQL翻译成MapReduce去Hadoop上执行，这样就使得数据开发和分析人员很方便的使用SQL来完成海量数据的统计和分析，而不必使用编程语言开发MapReduce那么麻烦首先本文主要讲解如何基于Hadoop搭建hive环境 1. 安装先进行下载安装包 http://mirror.bit.edu.cn/apache/hive/hive-3.1.1/ 我下载的是这个版本的，下载好apache-hive-3.1.1-bin.tar.gz 后拷贝到我的Linux服务器上 2. 配置3. 使用场景(1)日志分析：大部分互联网公司使用hive进行日志分析，包括百度、淘宝等。 1)统计网站一个时间段内的pv、uv 2)多维度数据分析 (2)海量结构化数据离线分析 可以查看这里的案例介绍]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>hive安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive]]></title>
    <url>%2F2019%2F02%2F11%2F1%2F</url>
    <content type="text"><![CDATA[优秀的人，不是不合群，而是他们合群的人里面没有你 什么是Hive,Hive和Hadoop的关系 Hive 环境安装以及 Hive使用场景 Hive系统架构 Hive 配置文件 Hive 命令行 HiveQL(语法、表查询)、Hive的数据存储https://www.jianshu.com/p/fedb3e7df6ce 数据库表设计 Hive优化 Hive 安全 Hive集成 Hive 性能 Hive案例]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>hive学习路线</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 3.2.0版本入门环境安装]]></title>
    <url>%2F2019%2F02%2F11%2FHadoop%20%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[本文主要讲解的是在单节点下的Hadoop的环境安装和配置，以帮助您可以快速的使用Hadoop的MapReduce和HDFS 1、下载Hadoop当前前提是你的Linux系统已经安装了Java，如果没有自行上网查找如何安装 Hadoop-3.2.0.tar.gz 2、 安装将下载好的压缩包解压到目标文件夹下，比如我解压的位置是：/usr/localhost/hadoop1tar -xzvf hadoop-3.2.0.tar.gz 3、配置环境变量 执行vi /etc/profile，在末尾增加hadoop的环境变量，也有网上教程说修改bash_profile文件，都可以，只是系统加载的优先级顺序不一样，如果不清楚可以上网进行查阅下， 例如： 123456789export HADOOP_HOME=/usr/local/hadoop/hadoop-3.2.0export HADOOP_INSTALL=$HADOOP_HOMEexport HADOOP_MAPRED_HOME=$HADOOP_HOMEexport HADOOP_COMMON_HOME=$HADOOP_HOMEexport HADOOP_HDFS_HOME=$HADOOP_HOMEexport YARN_HOME=$HADOOP_HOMEexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"export PATH=$PATH:$&#123;HADOOP_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/sbin 保存下profile文件 1$ source /etc/profile 4、修改配置修改Hadoop安装目录下etc/hadoop/ 中的core-site.xml、core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml，配置中的节点详解可以查询官网或者这篇文章 core-site.xml 大家改成自己的IP，也可以使用127.0.0.1，9000是默认端口 123456789101112131415161718192021222324&lt;configuration&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt; &lt;description&gt;hadoop运行时产生的文件的存储位置&lt;/description&gt;&lt;/property&gt;&lt;property&gt; #hadoop使用的文件系统(uri) hdfs 和hdfs的位置 &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://45.63.94.167:9000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;131702&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml 都有默认值 也可以不配置 12345678910111213141516171819202122&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml 123456789101112131415161718192021222324252627282930313233343536&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.auxservices.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;45.63.94.167:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;45.63.94.167:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;45.63.94.167:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;45.63.94.167:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;45.63.94.167:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;300&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改etc/hadoop/hadoop-env.sh 12#jdk 环境 因为要远程调用 $&#123;java_home&#125;找不到变量 export JAVA_HOME=/export/servers/jdk1.8.0_192 添加hdfs权限 vim sbin/start-dfs.sh sbin/stop-dfs.sh 在顶部空白位置添加 1234HDFS_DATANODE_USER=rootHDFS_DATANODE_SECURE_USER=hdfsHDFS_NAMENODE_USER=rootHDFS_SECONDARYNAMENODE_USER=root 否则会报错12345678ERROR: Attempting to launch hdfs namenode as rootERROR: but there is no HDFS_NAMENODE_USER defined. Aborting launch.Starting datanodesERROR: Attempting to launch hdfs datanode as rootERROR: but there is no HDFS_DATANODE_USER defined. Aborting launch.Starting secondary namenodes [localhost.localdomain]ERROR: Attempting to launch hdfs secondarynamenode as rootERROR: but there is no HDFS_SECONDARYNAMENODE_USER defined. Aborting launch. 添加yarn权限 vim sbin/start-yarn.sh sbin/stop-yarn.sh123YARN_RESOURCEMANAGER_USER=rootHDFS_DATANODE_SECURE_USER=yarnYARN_NODEMANAGER_USER=root 5、修改SSH，配置单机互信123ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsacat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keyschmod 0600 ~/.ssh/authorized_keys 6、启动先格式化HDFS 启动HDFS 然后启动yarn 6.1启动 ResourceManager守护进程和NodeManager守护进程: $ ./sbin/start-dfs.sh 停止$ ./sbin/stop-dfs.sh 启动yarn $HADOOP_HOME/sbin/start-yarn.sh 关闭yarn $HADOOP_HOME/sbin/stop-yarn.sh 6.2 批量操作1234启动所有start-all.sh 关闭所有stop-all.sh 验证是否启动成功可以使用jps1234567[root@yh01 logs]# jps5633 Jps4498 ResourceManager4020 DataNode3879 NameNode4247 SecondaryNameNode4635 NodeManager 6.3 访问web页面 Daemon Web Interface Notes NameNode http://nn_host:port/ Default HTTP port is 9870 ResourceManager http://rm_host:port/ Default HTTP port is 8088 MapReduce JobHistory Server http://jhs_host:port/ Default HTTP port is 19888 注意记得把端口在防火墙中允许或者关闭防火墙]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hadoop安装</tag>
      </tags>
  </entry>
</search>
